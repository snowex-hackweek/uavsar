{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "violent-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import datetime\n",
    "\n",
    "#database imports\n",
    "from snowexsql.db import get_db\n",
    "from snowexsql.data import PointData, LayerData, ImageData, SiteData\n",
    "from snowexsql.conversions import query_to_geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blank-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowexsql database loading successfull!\n"
     ]
    }
   ],
   "source": [
    "# load the database\n",
    "db_name = 'snow:hackweek@52.32.183.144/snowex'\n",
    "engine, session = get_db(db_name)\n",
    "\n",
    "print('snowexsql database loading successfull!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "associate-cheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of airborne sensors by \"surveyor\" name: \n",
      " [('USGS',), ('UAVSAR team, JPL',), ('ASO Inc.',)]\n"
     ]
    }
   ],
   "source": [
    "# Query the session using .surveyors() to generate a list\n",
    "qry = session.query(ImageData.surveyors)\n",
    "\n",
    "# Locate all that are distinct\n",
    "airborne_sensors_list = session.query(ImageData.surveyors).distinct().all()\n",
    "\n",
    "print('list of airborne sensors by \"surveyor\" name: \\n', airborne_sensors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blessed-capacity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAVSAR team, JPL flight dates are: 2020-01-31, 2020-02-12\n",
      "                             geom site_id        date\n",
      "0  POINT (740652.000 4327445.000)     2C2  2020-01-31\n",
      "1  POINT (744396.000 4323540.000)    8C26  2020-01-31\n",
      "2  POINT (741960.000 4326644.000)    6C10  2020-01-31\n",
      "3  POINT (741493.000 4326833.000)     1C8  2020-01-31\n",
      "4  POINT (745340.000 4322754.000)    8S28  2020-01-31\n"
     ]
    }
   ],
   "source": [
    "# Airborne sensor from list above\n",
    "sensor = 'UAVSAR team, JPL'\n",
    "\n",
    "# Form on the Images table that returns Raster collection dates\n",
    "qry = session.query(ImageData.date)\n",
    "\n",
    "# Filter for UAVSAR data\n",
    "qry = qry.filter(ImageData.surveyors == sensor)\n",
    "\n",
    "# Grab the unique dates\n",
    "qry = qry.distinct()\n",
    "\n",
    "# Execute the query \n",
    "dates = qry.all() \n",
    "\n",
    "# Clean up the dates \n",
    "dates = [d[0] for d in dates] \n",
    "dlist = [str(d) for d in dates]\n",
    "dlist = \", \".join(dlist)\n",
    "print('%s flight dates are: %s' %(sensor, dlist))\n",
    "\n",
    "# Find all the snow pits done on these days\n",
    "qry = session.query(SiteData.geom, SiteData.site_id, SiteData.date)\n",
    "qry = qry.filter(SiteData.date.in_(dates))\n",
    "\n",
    "# Return a geopandas df\n",
    "df = query_to_geopandas(qry, engine)\n",
    "\n",
    "# View the returned pandas dataframe!\n",
    "print(df.head())\n",
    "\n",
    "# Close your session to avoid hanging transactions\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satisfied-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 pits overlap with UAVSAR team, JPL on 2020-01-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geom</th>\n",
       "      <th>site_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (740652.000 4327445.000)</td>\n",
       "      <td>2C2</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (744396.000 4323540.000)</td>\n",
       "      <td>8C26</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (741960.000 4326644.000)</td>\n",
       "      <td>6C10</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (741493.000 4326833.000)</td>\n",
       "      <td>1C8</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (745340.000 4322754.000)</td>\n",
       "      <td>8S28</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             geom site_id        date\n",
       "0  POINT (740652.000 4327445.000)     2C2  2020-01-31\n",
       "1  POINT (744396.000 4323540.000)    8C26  2020-01-31\n",
       "2  POINT (741960.000 4326644.000)    6C10  2020-01-31\n",
       "3  POINT (741493.000 4326833.000)     1C8  2020-01-31\n",
       "4  POINT (745340.000 4322754.000)    8S28  2020-01-31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a day from the list of dates\n",
    "dt = dates[0] \n",
    "\n",
    "# Find all the snow pits done on these days \n",
    "qry = session.query(SiteData.geom, SiteData.site_id, SiteData.date)\n",
    "qry = qry.filter(SiteData.date == dt)\n",
    "\n",
    "# Return a geopandas df\n",
    "df_exact = query_to_geopandas(qry, engine)\n",
    "\n",
    "print('%s pits overlap with %s on %s' %(len(df_exact), sensor, dt))\n",
    "\n",
    "# View snows pits that align with first UAVSAR date\n",
    "df_exact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "biblical-peripheral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geom</th>\n",
       "      <th>site_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (740652.000 4327445.000)</td>\n",
       "      <td>2C2</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (744396.000 4323540.000)</td>\n",
       "      <td>8C26</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (741960.000 4326644.000)</td>\n",
       "      <td>6C10</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (741493.000 4326833.000)</td>\n",
       "      <td>1C8</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (745340.000 4322754.000)</td>\n",
       "      <td>8S28</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>POINT (744862.000 4323250.000)</td>\n",
       "      <td>4C30</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>POINT (742466.000 4324372.000)</td>\n",
       "      <td>2N12</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>POINT (744477.000 4323731.000)</td>\n",
       "      <td>8C22</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>POINT (740765.000 4327379.000)</td>\n",
       "      <td>2C3</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>POINT (741378.000 4326992.000)</td>\n",
       "      <td>1C7</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>POINT (740508.000 4327577.000)</td>\n",
       "      <td>1C1</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>POINT (744757.000 4323667.000)</td>\n",
       "      <td>6C24</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>POINT (741132.000 4327061.000)</td>\n",
       "      <td>2C6</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>POINT (742453.000 4325752.000)</td>\n",
       "      <td>1C14</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>POINT (740839.000 4327345.000)</td>\n",
       "      <td>2C4</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>POINT (741580.000 4326713.000)</td>\n",
       "      <td>2C9</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>POINT (745010.000 4323372.000)</td>\n",
       "      <td>9C28</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              geom site_id        date\n",
       "0   POINT (740652.000 4327445.000)     2C2  2020-01-31\n",
       "1   POINT (744396.000 4323540.000)    8C26  2020-01-31\n",
       "2   POINT (741960.000 4326644.000)    6C10  2020-01-31\n",
       "3   POINT (741493.000 4326833.000)     1C8  2020-01-31\n",
       "4   POINT (745340.000 4322754.000)    8S28  2020-01-31\n",
       "5   POINT (744862.000 4323250.000)    4C30  2020-01-31\n",
       "6   POINT (742466.000 4324372.000)    2N12  2020-01-31\n",
       "7   POINT (744477.000 4323731.000)    8C22  2020-01-31\n",
       "8   POINT (740765.000 4327379.000)     2C3  2020-01-31\n",
       "9   POINT (741378.000 4326992.000)     1C7  2020-01-31\n",
       "10  POINT (740508.000 4327577.000)     1C1  2020-01-31\n",
       "11  POINT (744757.000 4323667.000)    6C24  2020-01-31\n",
       "12  POINT (741132.000 4327061.000)     2C6  2020-01-31\n",
       "13  POINT (742453.000 4325752.000)    1C14  2020-01-31\n",
       "14  POINT (740839.000 4327345.000)     2C4  2020-01-31\n",
       "15  POINT (741580.000 4326713.000)     2C9  2020-01-31\n",
       "16  POINT (745010.000 4323372.000)    9C28  2020-01-31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "helpful-cabin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point data on 2020-01-31 are: camera, magnaprobe, pit ruler, with the following list of parameters: depth\n",
      "\n",
      "Layer Data on 2020-01-31 are: IRIS, snowmicropen, IS3-SP-11-01F, with the following list of parameters: sample_signal, force, grain_size, density, reflectance, permittivity, lwc_vol, manual_wetness, equivalent_diameter, specific_surface_area, grain_type, temperature, hand_hardness\n",
      "\n",
      "Image Data on 2020-01-31 are: UAVSAR, L-band InSAR, with the following list of parameters: insar amplitude\n"
     ]
    }
   ],
   "source": [
    "# Find all the data that was collected on 1-31-2020\n",
    "dt = datetime.date(2020, 1, 31)\n",
    "\n",
    "#--------------- Point Data -----------------------------------\n",
    "# Grab all Point data instruments from our date\n",
    "point_instruments = session.query(PointData.instrument).filter(PointData.date == dt).distinct().all()\n",
    "point_type = session.query(PointData.type).filter(PointData.date == dt).distinct().all()\n",
    "\n",
    "# Clean up point data (i.e. remove tuple)\n",
    "point_instruments = [p[0] for p in point_instruments]\n",
    "point_instruments = \", \".join(point_instruments)\n",
    "point_type = [p[0] for p in point_type]\n",
    "point_type = \", \".join(point_type)\n",
    "print('Point data on %s are: %s, with the following list of parameters: %s' %(str(dt), point_instruments, point_type))\n",
    "\n",
    "#--------------- Layer Data -----------------------------------\n",
    "# Grab all Layer data instruments from our date\n",
    "layer_instruments = session.query(LayerData.instrument).filter(LayerData.date == dt).distinct().all()\n",
    "layer_type = session.query(LayerData.type).filter(LayerData.date == dt).distinct().all()\n",
    "\n",
    "# Clean up layer data \n",
    "layer_instruments = [l[0] for l in layer_instruments if l[0] is not None]\n",
    "layer_instruments = \", \".join(layer_instruments)\n",
    "layer_type = [l[0] for l in layer_type]\n",
    "layer_type = \", \".join(layer_type)\n",
    "print('\\nLayer Data on %s are: %s, with the following list of parameters: %s' %(str(dt), layer_instruments, layer_type))\n",
    "\n",
    "#--------------- Image Data -----------------------------------\n",
    "# Grab all Image data instruments from our date\n",
    "image_instruments = session.query(ImageData.instrument).filter(ImageData.date == dt).distinct().all()\n",
    "image_type = session.query(ImageData.type).filter(ImageData.date == dt).distinct().all()\n",
    "\n",
    "# Clean up image data (i.e. remove tuple)\n",
    "image_instruments = [i[0] for i in image_instruments]\n",
    "image_instruments = \", \".join(image_instruments)\n",
    "image_type = [i[0] for i in image_type]\n",
    "image_type = \", \".join(image_type)\n",
    "print('\\nImage Data on %s are: %s, with the following list of parameters: %s' %(str(dt), image_instruments, image_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cellular-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_veg_class(site_id):\n",
    "    \n",
    "    '''\n",
    "    This function parses snow pit data into three vegetation classes:\n",
    "        - 1). Treeless, 2). Sparce, and 3). Dense\n",
    "        \n",
    "    It uses a python dictionary where:\n",
    "        (k) keys: are the vegetation classes\n",
    "        (v) values: are the first digit in the pitID assignment\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Classifying by vegetation coverage \n",
    "    veg_class = {'treeless':[1, 2, 3], 'sparse':[4, 5, 6], 'dense':[7, 8, 9]}\n",
    "     \n",
    "    vclass = None \n",
    "    \n",
    "    class_id = site_id[0]\n",
    "    \n",
    "    if class_id.isnumeric():\n",
    "        class_id = int(class_id)\n",
    "\n",
    "        for k,v in veg_class.items():\n",
    "\n",
    "            if class_id in v: #if the first digit in the site_id is 'v' assign it to the corresponding 'k'\n",
    "                vclass = k \n",
    "                \n",
    "    return vclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "narrative-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_depth_class(site_id):\n",
    "    \n",
    "    '''\n",
    "    This function parses snow pit data into three depth classes:\n",
    "        - 1). Shallow, 2). Medium, and 3). Deep\n",
    "        \n",
    "    It uses a python dictionary where:\n",
    "        (k) keys: are the depth classes\n",
    "        (v) values: are the first digit in the pitID assignment\n",
    "      \n",
    "  \n",
    "    '''\n",
    "        \n",
    "    # Classifying by 2017 depth \n",
    "    depth_class = {'shallow':[1, 4, 7], 'medium':[2, 5, 8], 'deep':[3, 6, 9]} \n",
    "   \n",
    "    dclass = None \n",
    "    \n",
    "    class_id = site_id[0]\n",
    "    \n",
    "    if class_id.isnumeric(): #for the outlier TS site\n",
    "        class_id = int(class_id) #cast as integer\n",
    "\n",
    "        for k,v in depth_class.items(): #for the key, value pairs in the dict listed above:\n",
    "\n",
    "            if class_id in v: #if the first digit in the site_id is 'v' assign it to the corresponding 'k'\n",
    "                dclass = k \n",
    "\n",
    "    return dclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "checked-manhattan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>value</th>\n",
       "      <th>veg_class</th>\n",
       "      <th>depth_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>8N35</td>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>density</td>\n",
       "      <td>39.032460</td>\n",
       "      <td>-108.170096</td>\n",
       "      <td>107.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>dense</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>1N3</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>density</td>\n",
       "      <td>39.033787</td>\n",
       "      <td>-108.218511</td>\n",
       "      <td>67.0</td>\n",
       "      <td>242.5</td>\n",
       "      <td>treeless</td>\n",
       "      <td>shallow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>9C16</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>density</td>\n",
       "      <td>39.044273</td>\n",
       "      <td>-108.197050</td>\n",
       "      <td>79.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>dense</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>6N46</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>density</td>\n",
       "      <td>39.028131</td>\n",
       "      <td>-108.151811</td>\n",
       "      <td>47.0</td>\n",
       "      <td>284.5</td>\n",
       "      <td>sparse</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>3S14</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>density</td>\n",
       "      <td>39.019233</td>\n",
       "      <td>-108.185671</td>\n",
       "      <td>103.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>treeless</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     site_id        date     type   latitude   longitude  depth  value  \\\n",
       "1412    8N35  2020-02-10  density  39.032460 -108.170096  107.0  201.0   \n",
       "1170     1N3  2020-02-11  density  39.033787 -108.218511   67.0  242.5   \n",
       "1755    9C16  2020-02-05  density  39.044273 -108.197050   79.0  271.0   \n",
       "2011    6N46  2020-02-01  density  39.028131 -108.151811   47.0  284.5   \n",
       "930     3S14  2020-02-01  density  39.019233 -108.185671  103.0  219.0   \n",
       "\n",
       "     veg_class depth_class  \n",
       "1412     dense      medium  \n",
       "1170  treeless     shallow  \n",
       "1755     dense        deep  \n",
       "2011    sparse        deep  \n",
       "930   treeless        deep  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the database\n",
    "db_name = 'snow:hackweek@52.32.183.144/snowex'\n",
    "engine, session = get_db(db_name)\n",
    "\n",
    "# Query for Layer Data\n",
    "result = session.query(LayerData.type).distinct().all()\n",
    "\n",
    "# Filter for density data\n",
    "qry = session.query(LayerData).filter(LayerData.type=='density')\n",
    "\n",
    "# Form our dataframe from the query \n",
    "df = query_to_geopandas(qry, engine)\n",
    "df['value'] = df['value'].astype(float)  #cast the value as a float (they are strings)\n",
    "\n",
    "# Parse snow pit data by the veg/depth matrix\n",
    "df['veg_class'] = [parse_veg_class(i) for i in df['site_id']] #run the parse_veg function for every site_id\n",
    "df['depth_class'] = [parse_depth_class(i) for i in df['site_id']] #run the parse_depth funciton for every site_id\n",
    "\n",
    "# Select columns of interest\n",
    "col_list = ['site_id', 'date', 'type', 'latitude',\n",
    "       'longitude', 'depth', 'value', 'veg_class', 'depth_class']\n",
    "df = df[col_list]\n",
    "\n",
    "# View a sample --> notice parsed veg_class and depth_class columns were added!\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sustained-boxing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       9C17\n",
       "1       9C17\n",
       "2       9C17\n",
       "3       9C17\n",
       "4       9C17\n",
       "        ... \n",
       "2817    3N26\n",
       "2818    3N26\n",
       "2819    3N26\n",
       "2820    1C14\n",
       "2821    1C14\n",
       "Name: site_id, Length: 2822, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "international-airport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2020-01-30\n",
       "1       2020-01-30\n",
       "2       2020-01-30\n",
       "3       2020-01-30\n",
       "4       2020-01-30\n",
       "           ...    \n",
       "2817    2020-02-08\n",
       "2818    2020-02-08\n",
       "2819    2020-02-08\n",
       "2820    2020-01-31\n",
       "2821    2020-01-31\n",
       "Name: date, Length: 2822, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "portable-murder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veg_class\n",
      "dense       50\n",
      "sparse      39\n",
      "treeless    60\n",
      "Name: site_id, dtype: int64\n",
      "\n",
      "\n",
      "depth_class\n",
      "deep       45\n",
      "medium     78\n",
      "shallow    26\n",
      "Name: site_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group by site-id to count classes\n",
    "gb = df.groupby(['site_id', 'veg_class', 'depth_class'])\n",
    "\n",
    "print(gb['site_id'].count().groupby('veg_class').count())\n",
    "print('\\n')\n",
    "print(gb['site_id'].count().groupby('depth_class').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "painful-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import zipfile\n",
    "import getpass\n",
    "from osgeo import gdal \n",
    "import os  # for chdir, getcwd, path.basename, path.exists\n",
    "import pandas as pd # for DatetimeIndex\n",
    "import codecs # for text parsing code\n",
    "import netrc\n",
    "import rasterio as rio\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecological-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NASA EARTHDATA Credentials from ~/.netrc or manual input\n",
    "try:\n",
    "    os.chmod('/home/jovyan/.netrc', 0o600) #only necessary on jupyterhub\n",
    "    (ASF_USER, account, ASF_PASS) = netrc.netrc().authenticators(\"urs.earthdata.nasa.gov\")\n",
    "except:\n",
    "    ASF_USER = input(\"Enter Username: \")\n",
    "    ASF_PASS = getpass.getpass(\"Enter Password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accompanied-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory:  /home/jovyan/uavsar/ShazWork\n",
      "/tmp\n"
     ]
    }
   ],
   "source": [
    "# directory in which the notebook resides\n",
    "if 'tutorial_home_dir' not in globals():\n",
    "     tutorial_home_dir = os.getcwd()\n",
    "print(\"Notebook directory: \", tutorial_home_dir)\n",
    "\n",
    "if not os.path.exists('/tmp/'):\n",
    "    os.chdir('/tmp')\n",
    "   \n",
    "# directory for data downloads\n",
    "\n",
    "data_dir = os.path.join('/tmp')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "departmental-split",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/srv/conda/envs/notebook/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3427\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-21-921727f4f92a>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', '\\nfiles = [\\'https://datapool.asf.alaska.edu/INTERFEROMETRY/UA/grmesa_27416_20003-028_20008-004_0018d_s01_L090_01_int.zip,]\\n    \\nfor file in files:\\n    print(f\\'downloading {file}...\\')\\n    filename = os.path.basename(file)\\n    \\n    if not os.path.exists(os.path.join(data_dir,filename)):\\n        cmd = \"wget -q {0} --user={1} --password={2} -P {3} -nc\".format(file, ASF_USER, ASF_PASS, data_dir)\\n        os.system(cmd)\\n    else:\\n        print(filename + \" already exists. Skipping download ..\")\\nprint(\"done\")\\n')\n",
      "  File \u001b[1;32m\"/srv/conda/envs/notebook/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2391\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[1;32m\"<decorator-gen-54>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n",
      "  File \u001b[1;32m\"/srv/conda/envs/notebook/lib/python3.8/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/srv/conda/envs/notebook/lib/python3.8/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1277\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/srv/conda/envs/notebook/lib/python3.8/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m101\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    files = ['https://datapool.asf.alaska.edu/INTERFEROMETRY/UA/grmesa_27416_20003-028_20008-004_0018d_s01_L090_01_int.zip,]\u001b[0m\n\u001b[0m                                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "files = ['https://datapool.asf.alaska.edu/INTERFEROMETRY/UA/grmesa_27416_20003-028_20008-004_0018d_s01_L090_01_int.zip,]\n",
    "    \n",
    "for file in files:\n",
    "    print(f'downloading {file}...')\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir,filename)):\n",
    "        cmd = \"wget -q {0} --user={1} --password={2} -P {3} -nc\".format(file, ASF_USER, ASF_PASS, data_dir)\n",
    "        os.system(cmd)\n",
    "    else:\n",
    "        print(filename + \" already exists. Skipping download ..\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "interior-burden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE GrMesa_2015_054_261_0006d/\n",
      "                           PRE GrMesa_2015_055_081_0008d/\n",
      "                           PRE GrMesa_2015_055_261_0008d/\n",
      "                           PRE GrMesa_2015_060_081_0048d/\n",
      "                           PRE GrMesa_2015_060_261_0048d/\n",
      "                           PRE GrMesa_2015_100_081_0360d/\n",
      "                           PRE GrMesa_2016_095_081_0313d/\n",
      "                           PRE GrMesa_2016_095_261_0313d/\n",
      "                           PRE grmesa_2017_002_078_0016d/\n",
      "                           PRE grmesa_2017_002_078_0019d/\n",
      "                           PRE grmesa_2017_002_078_0030d/\n",
      "                           PRE grmesa_2017_002_078_0053d/\n",
      "                           PRE grmesa_2017_002_080_0016d/\n",
      "                           PRE grmesa_2017_002_080_0030d/\n",
      "                           PRE grmesa_2017_002_080_0053d/\n",
      "                           PRE grmesa_2017_002_260_0016d/\n",
      "                           PRE grmesa_2017_002_260_0019d/\n",
      "                           PRE grmesa_2017_002_260_0030d/\n",
      "                           PRE grmesa_2017_002_260_0053d/\n",
      "                           PRE grmesa_2017_016_078_0003d/\n",
      "                           PRE grmesa_2017_016_078_0014d/\n",
      "                           PRE grmesa_2017_016_078_0037d/\n",
      "                           PRE grmesa_2017_016_080_0014d/\n",
      "                           PRE grmesa_2017_016_080_0037d/\n",
      "                           PRE grmesa_2017_016_260_0003d/\n",
      "                           PRE grmesa_2017_016_260_0014d/\n",
      "                           PRE grmesa_2017_016_260_0037d/\n",
      "                           PRE grmesa_2017_018_078_0034d/\n",
      "                           PRE grmesa_2017_018_260_0011d/\n",
      "                           PRE grmesa_2017_018_260_0034d/\n",
      "                           PRE grmesa_2020_003_274_0011d/\n",
      "                           PRE grmesa_2020_003_274_0018d/\n",
      "                           PRE grmesa_2020_003_274_0025d/\n",
      "                           PRE grmesa_2020_003_274_0040d/\n",
      "                           PRE grmesa_2020_005_274_0007d/\n",
      "                           PRE grmesa_2020_008_274_0007d/\n",
      "                           PRE grmesa_2020_013_274_0015d/\n",
      "                           PRE grmesa_2021_007_274_0007d/\n",
      "                           PRE grmesa_2021_010_274_0007d/\n",
      "                           PRE grmesa_2021_011_274_0021d/\n",
      "                           PRE grmesa_2021_016_274_0007d/\n",
      "                           PRE grmesa_2021_017_274_0006d/\n",
      "                           PRE grmesa_2021_019_274_0006d/\n",
      "                           PRE lowman_2019_084_232_0042d/\n",
      "                           PRE lowman_2020_002_232_0013d/\n",
      "                           PRE lowman_2020_007_232_0008d/\n",
      "                           PRE lowman_2020_011_232_0019d/\n",
      "                           PRE peeler_2019_084_316_0054d/\n",
      "                           PRE peeler_2020_005_316_0007d/\n",
      "                           PRE peeler_2020_008_316_0007d/\n",
      "                           PRE peeler_2020_013_316_0015d/\n",
      "                           PRE rockmt_2020_003_141_0011d/\n",
      "                           PRE rockmt_2020_005_141_0007d/\n",
      "                           PRE rockmt_2020_005_321_0007d/\n",
      "                           PRE rockmt_2020_008_141_0007d/\n",
      "                           PRE rockmt_2020_008_321_0022d/\n",
      "                           PRE rockmt_2020_013_141_0015d/\n",
      "                           PRE silver_2020_011_347_0019d/\n",
      "                           PRE uticam_2021_002_210_0005d/\n",
      "                           PRE uticam_2021_004_210_0034d/\n",
      "2021-07-14 02:24:30          0 test.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://snowex-data/uavsar-project/UAVSAR_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "latin-cream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal error: An error occurred (404) when calling the HeadObject operation: Key \"uavsar-project/UAVSAR_images/grmesa_2020_003_274_0011d/grmesa_27416_20003-028_20005-007_0011d_s01_L090HH_01.cor.grd.tiff\" does not exist\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://snowex-data/uavsar-project/UAVSAR_images/grmesa_2020_003_274_0011d/grmesa_27416_20003-028_20005-007_0011d_s01_L090HH_01.cor.grd.tiff /tmp/grmesa_27416_20003-028_20005-007_0011d_s01_L090HH_01.cor.grd.tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-candle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
