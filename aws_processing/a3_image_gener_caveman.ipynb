{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "responsible-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import zipfile\n",
    "import getpass\n",
    "from osgeo import gdal \n",
    "import os  # for chdir, getcwd, path.basename, path.exists\n",
    "import pandas as pd # for DatetimeIndex\n",
    "import codecs # for text parsing code\n",
    "import netrc\n",
    "import rasterio as rio\n",
    "import glob\n",
    "import io\n",
    "import shutil\n",
    "from subprocess import PIPE, Popen\n",
    "import subprocess\n",
    "import fcntl ##may need to pip install this one\n",
    "import select\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intended-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloading(file, amp=False):\n",
    "    \"\"\"\n",
    "    Downloads and unzips UAVSAR images from ASF Vertex. Only tested on .GRD Interferometric Pairs. \n",
    "    Ideally for this application only pass 1 url at a time.\n",
    "    :param zip_url: url pointing at a UAVSAR .zip file\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Get NASA EARTHDATA Credentials from ~/.netrc or manual input\n",
    "    try:\n",
    "        os.chmod('/home/jovyan/.netrc', 0o600) #only necessary on jupyterhub\n",
    "        (ASF_USER, account, ASF_PASS) = netrc.netrc().authenticators(\"urs.earthdata.nasa.gov\")\n",
    "    except:\n",
    "        ASF_USER = input(\"Enter Username: \")\n",
    "        ASF_PASS = getpass.getpass(\"Enter Password: \")\n",
    "        \n",
    "        \n",
    "    data_dir = '/tmp/'\n",
    "   \n",
    "    # directory for data downloads\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.chdir(data_dir)\n",
    "\n",
    "    filename = os.path.basename(file)\n",
    "\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir,filename)):\n",
    "        print(f'downloading {file}...')\n",
    "        \n",
    "        cmd = \"wget -1 {0} --user={1} --password={2} -P {3} --progress=bar:force\".format(file, ASF_USER, ASF_PASS, data_dir)\n",
    "        #os.system(cmd) \n",
    "        #subprocess.call(cmd)\n",
    "        process = Popen(['wget',file,'--user={}'.format(ASF_USER),'--password={}'.format(ASF_PASS),'-P',data_dir,'--progress=bar'], stderr=subprocess.PIPE)\n",
    "        started = False\n",
    "        for line in process.stderr:\n",
    "            line = line.decode(\"utf-8\", \"replace\")\n",
    "            if started:\n",
    "                splited = line.split()\n",
    "                if len(splited) == 9:\n",
    "                    percentage = splited[6]\n",
    "                    speed = splited[7]\n",
    "                    remaining = splited[8]\n",
    "                    print(\"Downloaded {} with {} per second and {} left.\".format(percentage, speed, remaining), end='\\r')\n",
    "            elif line == os.linesep:\n",
    "                started = True\n",
    "\n",
    "        ##Should probably be a subprocess.call(cmd) - not quite sure why but that is the perfered method\n",
    "    else:\n",
    "        print(filename + \" already exists. Skipping download ..\")\n",
    "\n",
    "        \n",
    "    if amp:\n",
    "        file = file.replace('int','amp').replace('INTERFEROMETRY_GRD','AMPLITUDE_GRD')\n",
    "        filename = os.path.basename(file)\n",
    "        if not os.path.exists(os.path.join(data_dir,filename)):\n",
    "            print(f'downloading {file}...')\n",
    "\n",
    "            cmd = \"wget -1 {0} --user={1} --password={2} -P {3} --progress=bar:force\".format(file, ASF_USER, ASF_PASS, data_dir)\n",
    "            #os.system(cmd) \n",
    "            #subprocess.call(cmd)\n",
    "            process = Popen(['wget',file,'--user={}'.format(ASF_USER),'--password={}'.format(ASF_PASS),'-P',data_dir,'--progress=bar'], stderr=subprocess.PIPE)\n",
    "            started = False\n",
    "            for line in process.stderr:\n",
    "                line = line.decode(\"utf-8\", \"replace\")\n",
    "                if started:\n",
    "                    splited = line.split()\n",
    "                    if len(splited) == 9:\n",
    "                        percentage = splited[6]\n",
    "                        speed = splited[7]\n",
    "                        remaining = splited[8]\n",
    "                        print(\"Downloaded {} with {} per second and {} left.\".format(percentage, speed, remaining), end='\\r')\n",
    "                elif line == os.linesep:\n",
    "                    started = True\n",
    "\n",
    "            ##Should probably be a subprocess.call(cmd) - not quite sure why but that is the perfered method\n",
    "        else:\n",
    "            print(filename + \" already exists. Skipping download ..\")\n",
    "    print(\"\\nDone\")\n",
    "    \n",
    "    # unzip\n",
    "\n",
    "    for file in glob.glob(\"/tmp/*.zip\"):\n",
    "        with zipfile.ZipFile(file, \"r\") as zip_ref:\n",
    "            print('Extracting all the files now...')\n",
    "            zip_ref.extractall('/tmp')\n",
    "            print(\"done\")\n",
    "    \n",
    "    return data_dir\n",
    "\n",
    "# folder is path to a folder with an .ann (or .txt) and .grd files (.amp1, .amp2, .cor, .unw, .int)\n",
    "\n",
    "def uavsar_tiff_convert(folder, verbose = False):\n",
    "    \"\"\"\n",
    "    Builds a header file for the input UAVSAR .grd file,\n",
    "    allowing the data to be read as a raster dataset.\n",
    "    :param folder:   the folder containing the UAVSAR .grd and .ann files\n",
    "    \"\"\"\n",
    "\n",
    "    os.chdir(folder)\n",
    "    int_file = glob.glob(os.path.join(folder, 'int.grd'))\n",
    "\n",
    "    # Empty lists to put information that will be recalled later.\n",
    "    Lines_list = []\n",
    "    Samples_list = []\n",
    "    Latitude_list = []\n",
    "    Longitude_list = []\n",
    "    Files_list = []\n",
    "\n",
    "    # Step 1: Look through folder and determine how many different flights there are\n",
    "    # by looking at the HDR files.\n",
    "    for files in os.listdir(folder):\n",
    "        if files [-4:] == \".grd\":\n",
    "            newfile = open(files[0:-4] + \".hdr\", 'w')\n",
    "            newfile.write(\"\"\"ENVI\n",
    "description = {DESCFIELD}\n",
    "samples = NSAMP\n",
    "lines = NLINE\n",
    "bands = 1\n",
    "header offset = 0\n",
    "data type = DATTYPE\n",
    "interleave = bsq\n",
    "sensor type = UAVSAR L-Band\n",
    "byte order = 0\n",
    "map info = {Geographic Lat/Lon, \n",
    "            1.000, \n",
    "            1.000, \n",
    "            LON, \n",
    "            LAT,  \n",
    "            0.0000555600000000, \n",
    "            0.0000555600000000, \n",
    "            WGS-84, units=Degrees}\n",
    "wavelength units = Unknown\n",
    "                \"\"\"\n",
    "                          )\n",
    "            newfile.close()\n",
    "            if files[0:18] not in Files_list:\n",
    "                Files_list.append(files[0:18])\n",
    "\n",
    "    #Variables used to recall indexed values.\n",
    "    var1 = 0\n",
    "\n",
    "    #Step 2: Look through the folder and locate the annotation file(s).\n",
    "    # These can be in either .txt or .ann file types.\n",
    "    for files in os.listdir(folder):\n",
    "        if Files_list[var1] and files[-4:] == \".txt\" or files[-4:] == \".ann\":\n",
    "            #Step 3: Once located, find the info we are interested in and append it to\n",
    "            # the appropriate list. We limit the variables to <=1 so that they only\n",
    "            # return two values (one for each polarization of\n",
    "            searchfile = codecs.open(files, encoding = 'windows-1252', errors='ignore')\n",
    "            for line in searchfile:\n",
    "                if \"Ground Range Data Latitude Lines\" in line:\n",
    "                    Lines = line[65:70]\n",
    "                    if verbose:\n",
    "                        print(f\"Number of Lines: {Lines}\")\n",
    "                    if Lines not in Lines_list:\n",
    "                        Lines_list.append(Lines)\n",
    "\n",
    "                elif \"Ground Range Data Longitude Samples\" in line:\n",
    "                    Samples = line[65:70]\n",
    "                    if verbose:\n",
    "                        print(f\"Number of Samples: {Samples}\")\n",
    "                    if Samples not in Samples_list:\n",
    "                        Samples_list.append(Samples)\n",
    "\n",
    "                elif \"Ground Range Data Starting Latitude\" in line:\n",
    "                    Latitude = line[65:85]\n",
    "                    if verbose:\n",
    "                        print(f\"Top left lat: {Latitude}\")\n",
    "                    if Latitude not in Latitude_list:\n",
    "                        Latitude_list.append(Latitude)\n",
    "\n",
    "                elif \"Ground Range Data Starting Longitude\" in line:\n",
    "                    Longitude = line[65:85]\n",
    "                    if verbose:\n",
    "                        print(f\"Top left Lon: {Longitude}\")\n",
    "                    if Longitude not in Longitude_list:\n",
    "                        Longitude_list.append(Longitude)\n",
    "    \n",
    "                        \n",
    "                 \n",
    "            #Reset the variables to zero for each different flight date.\n",
    "            var1 = 0\n",
    "            searchfile.close()\n",
    "\n",
    "\n",
    "    # Step 3: Open .hdr file and replace data for all type 4 (real numbers) data\n",
    "    # this all the .grd files expect for .int\n",
    "    for files in os.listdir(folder):\n",
    "        if files[-4:] == \".hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = DATTYPE\" in line:\n",
    "                        sources.write(re.sub(line[12:19], \"4\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "    \n",
    "    # Step 3: Open .hdr file and replace data for .int file date type 6 (complex)                 \n",
    "    for files in os.listdir(folder):\n",
    "        if files[-8:] == \".int.hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = 4\" in line:\n",
    "                        sources.write(re.sub(line[12:13], \"6\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "                        \n",
    "    \n",
    "    # Step 4: Now we have an .hdr file, the data is geocoded and can be loaded into python with rasterio\n",
    "    # once loaded in we use gdal.Translate to convert and save as a .tiff\n",
    "    \n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.grd')) # list all .grd files\n",
    "    for data_path in data_to_process: # loop to open and translate .grd to .tiff, and save .tiffs using gdal\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_Float32)\n",
    "    \n",
    "    # Step 5: Save the .int raster, needs separate save because of the complex format\n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.int.grd')) # list all .int.grd files (only 1)\n",
    "    for data_path in data_to_process:\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_CFloat32)\n",
    "\n",
    "    print(\".tiffs have been created\")\n",
    "    return\n",
    "\n",
    "def a3_bucket_transfer(folder):\n",
    "    \"\"\"\n",
    "    transfers converted .tiff files to the a3 cloud\n",
    "    :param folder:  (filepath) to folder containing the UAVSAR .tiff\n",
    "    :param region: (string) the region the flight is from\n",
    "    \"\"\"\n",
    "    num_tiffs = len(glob.glob(folder+ \"*.tiff\"))\n",
    "    \n",
    "    for tiff in glob.glob(folder+ \"*.tiff\"):\n",
    "        base_name = tiff.split('/')[-1]\n",
    "        \n",
    "        region = base_name.split('_')[0]\n",
    "        year = '20' + base_name.split('_')[2][0:2]\n",
    "        flight_num = base_name.split('_')[2][2:5]\n",
    "        flight_heading = base_name.split('_')[1][0:3]\n",
    "        days_between = base_name.split('_')[4]\n",
    "        folder_name = '{}_{}_{}_{}_{}/'.format(region,year,flight_num, flight_heading, days_between)\n",
    "        \n",
    "        tiff_folder_fp = os.path.join(folder, folder_name)\n",
    "        if not os.path.exists(tiff_folder_fp):\n",
    "            os.mkdir(tiff_folder_fp)\n",
    "        \n",
    "\n",
    "        os.replace(tiff,tiff_folder_fp+base_name )\n",
    "    cmd = 'aws s3 cp {} s3://snowex-data/uavsar-project/UAVSAR_images/{} --recursive'.format(tiff_folder_fp,folder_name)\n",
    "    os.system(cmd)\n",
    "    \n",
    "    ###now check for upload complete (probably a smoother way to do this. This is has a danger of hanging)\n",
    "    upload_incomplete = True\n",
    "    if upload_incomplete:\n",
    "        cmd = 'aws s3 ls s3://snowex-data/uavsar-project/UAVSAR_images/{} | wc -l'.format(folder_name)\n",
    "        stream = os.popen(cmd)\n",
    "        output = stream.read()\n",
    "        if int(output) == num_tiffs:\n",
    "            upload_incomplete = False\n",
    "            print('upload complete')\n",
    "    \n",
    "    return folder_name, output\n",
    "\n",
    "def clear_folder(folder):\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder, ignore_errors = True)\n",
    "    return\n",
    "\n",
    "def main(zip_url, clear_temp = True, amp =False):\n",
    "    zip_num = len(zip_url)\n",
    "    count = 1\n",
    "    for url in zip_url:\n",
    "        print('Starting {} of {} zips'.format(count, zip_num))\n",
    "        count += 1\n",
    "        data_dir = downloading(url, amp = amp)\n",
    "        uavsar_tiff_convert(data_dir)\n",
    "        folder_name, number_uploaded = a3_bucket_transfer(data_dir)\n",
    "        if clear_temp:\n",
    "            clear_folder(data_dir)\n",
    "        print('Created folder {} with {} images'.format(folder_name, number_uploaded.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "casual-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/tmp/'\n",
    "clear_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/silver_34715_20011-001_20016-002_0019d_s01_L090_01_int_grd.zip'\n",
    "downloading(test_file)\n",
    "\n",
    "folder = '/tmp/'\n",
    "uavsar_tiff_convert(folder)\n",
    "\n",
    "folder = '/tmp/'\n",
    "a3_bucket_transfer(folder)\n",
    "\n",
    "folder = '/tmp/'\n",
    "clear_folder(folder)\n",
    "\n",
    "test_files = ['https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/grmesa_27416_21019-017_21021-005_0006d_s01_L090_01_int_grd.zip']\n",
    "main(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-pleasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 of 15 zips\n",
      "downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_20013-003_20017-008_0015d_s01_L090_01_int_grd.zip...\n",
      "Downloaded 39% with 17.4M per second and 3m3s left..\r"
     ]
    }
   ],
   "source": [
    "test_files = ['https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/grmesa_27416_21019-017_21021-005_0006d_s01_L090_01_int_grd.zip']\n",
    "main(HP_INT_RQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "silver-endorsement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_20013-003_20017-008_0015d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_32109_20008-009_20017-003_0022d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_14107_20013-006_20017-002_0015d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/silver_34715_20011-001_20016-002_0019d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_14107_20008-006_20013-006_0007d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_20008-003_20013-003_0007d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_32109_20005-012_20008-009_0007d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/lowman_23205_20007-003_20011-003_0008d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/lowman_23205_20011-003_20016-004_0019d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_14107_20005-009_20008-006_0007d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_20005-006_20008-003_0007d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/lowman_23205_20002-007_20007-003_0013d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_14107_20003-000_20005-009_0011d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_19084-021_20005-006_0054d_s01_L090_01_int_grd.zip\n",
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/lowman_23205_19084-017_20002-007_0042d_s01_L090_01_int_grd.zip\n",
      "['https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_20013-003_20017-008_0015d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_32109_20008-009_20017-003_0022d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_14107_20013-006_20017-002_0015d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/silver_34715_20011-001_20016-002_0019d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_14107_20008-006_20013-006_0007d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_20008-003_20013-003_0007d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_32109_20005-012_20008-009_0007d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/lowman_23205_20007-003_20011-003_0008d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/lowman_23205_20011-003_20016-004_0019d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_14107_20005-009_20008-006_0007d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_20005-006_20008-003_0007d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/lowman_23205_20002-007_20007-003_0013d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/rockmt_14107_20003-000_20005-009_0011d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/peeler_31619_19084-021_20005-006_0054d_s01_L090_01_int_grd.zip', 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/lowman_23205_19084-017_20002-007_0042d_s01_L090_01_int_grd.zip']\n"
     ]
    }
   ],
   "source": [
    "grand_mesa_txt = open('/home/jovyan/uavsar/aws_processing/HP_Requests.txt')\n",
    "content = grand_mesa_txt.read()\n",
    "grand_mesa_zips = content.split(\"\\n\")\n",
    "grand_mesa_txt.close()\n",
    "HP_INT_RQ = []\n",
    "for i in grand_mesa_zips:\n",
    "    print(i)\n",
    "    if ('int_grd') in i:\n",
    "        HP_INT_RQ.append(i)\n",
    "print(HP_INT_RQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-period",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(grand_mesa_zips)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
